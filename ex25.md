1. 小规模,数量小爬取速度不敏感    requests库 爬取网页
2. 中规模,数据大爬取速度敏感      scrapy库  爬取网站
3. 大规模,搜素引擎爬速度关键      定制      爬取全网

爬虫限制

来源审查:判断User-Agent进行限制. 检查来访HTTP协议头的User-Agent,只响应浏览器或者友好爬虫的访问.
发布公告:Robots 协议
告知所有爬虫网站得爬取策略,要求爬虫遵守.

headers 修改 user-agent

# beautiful soup库

tag = soup.a (标签)
tag.attrs (属性)
tag.attrs['class']
soup.a.string  a标签之 间得属性

bs4遍历html
contents 列表 []
children 

for parent in soup.a.parents:
  if parent is None:
  print(parent)
  else:
  print(parent.name)

next_sibling  平行下一个兄弟标签
previous_sibling 前一个平行节点

soup.prettify() 给每个标签加换行副(格式化)

# scrapy 
spiders  输入URL 解析获的的内容
item pipelenes  获取数据处理

engine 控制所有模块之间得数据流   根据条件出发
downloader 根据请求下载网页
scheduler 对所有爬取请求进行调度管理

Downloader Middleware 
目的:实施Engine /Scheduler/downloader 之间进行用户可配置的控制
功能:修改/丢弃/新增请求或响应

Spider Middleware
目的:对请求和爬取项的再处理
功能:修改/丢弃/新增请求或者爬取项

常用命令
startproject 创建新工程
genspider 创建一个爬虫
settings 获得爬虫配置信息
crawl 运行爬虫
list 列出工程中所有爬虫
shell 启动URL调试命令